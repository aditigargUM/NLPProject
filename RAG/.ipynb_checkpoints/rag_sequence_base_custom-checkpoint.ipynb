{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6d6c21-ffb9-4fd9-84c3-1ca26dc44b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration, AutoTokenizer, T5Tokenizer\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4bef87-77c2-4cef-81a0-3500ed344246",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff69993-655b-4a76-a75c-32b9c9004454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = RagSequenceForGeneration.from_pretrained_question_encoder_generator(\"facebook/dpr-question_encoder-single-nq-base\", \"t5-small\")\n",
    "    question_encoder_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "    generator_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\") \n",
    "    # this had to replaced to a smaller model compared to the original BART-large, \n",
    "    # probably that was causing CUDA out of memory on a small GPU\n",
    "\n",
    "    tokenizer = RagTokenizer(question_encoder_tokenizer, generator_tokenizer)\n",
    "    model.config.use_dummy_dataset = True # use dummy dataset for POC\n",
    "    model.config.index_name = \"exact\"\n",
    "    retriever = RagRetriever(model.config, question_encoder_tokenizer, generator_tokenizer)\n",
    "    \n",
    "    model.set_retriever(retriever)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return (model, tokenizer, retriever)\n",
    "\n",
    "def infer(model, tokenizer, retriever):\n",
    "    input_dict = tokenizer.prepare_seq2seq_batch(\"who holds the record in 100m freestyle\", \"michael phelps\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model(input_dict[\"input_ids\"], labels=input_dict[\"labels\"])\n",
    "\n",
    "    loss = outputs.loss\n",
    "    print(\"loss: \", loss)\n",
    "    \n",
    "def save_model(model, tokenizer, retriever, path=\"./rag_model_custom\"):\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "    retriever.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11f5035-9949-4d71-b510-a724f41d6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_custom():\n",
    "    model = RagSequenceForGeneration.from_pretrained_question_encoder_generator(\"facebook/dpr-ctx_encoder-single-nq-base\", \"t5-small\")\n",
    "    question_encoder_tokenizer = BertTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "    generator_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "    tokenizer = RagTokenizer(question_encoder_tokenizer, generator_tokenizer)\n",
    "    model.config.use_dummy_dataset = True # use dummy dataset for POC\n",
    "    model.config.index_name = \"exact\"\n",
    "    retriever = RagRetriever(model.config, question_encoder_tokenizer, generator_tokenizer)\n",
    "    \n",
    "    model.set_retriever(retriever)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return (model, tokenizer, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae7c57-85eb-4655-b629-0bdda6bdd243",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model, tokenizer, retriever) = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef6017b-73e1-4783-96c0-83505b7f2051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gdhanania_umass_edu/.conda/envs/cont-base/lib/python3.10/site-packages/transformers/models/rag/tokenization_rag.py:92: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n",
      "2023-04-28 06:03:28.384254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-28 06:03:31.633239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor([28.8410], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "infer(model, tokenizer, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36865939-0880-40ec-8f6a-cced7648cbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel1 = RagSequenceForGeneration.from_pretrained(\"./rag_model_custom\")\\n\\nquestion_encoder_tokenizer1 = AutoTokenizer.from_pretrained(\"./rag_model_custom\")\\ngenerator_tokenizer1 = T5.from_pretrained(\"./rag_model_custom\")\\n\\ntokenizer1 = RagTokenizer(question_encoder_tokenizer1, generator_tokenizer1)\\nretriever1 = RagRetriever(model.config, question_encoder_tokenizer1, generator_tokenizer1)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model1 = RagSequenceForGeneration.from_pretrained(\"./rag_model_custom\")\n",
    "\n",
    "question_encoder_tokenizer1 = AutoTokenizer.from_pretrained(\"./rag_model_custom\")\n",
    "generator_tokenizer1 = T5.from_pretrained(\"./rag_model_custom\")\n",
    "\n",
    "tokenizer1 = RagTokenizer(question_encoder_tokenizer1, generator_tokenizer1)\n",
    "retriever1 = RagRetriever(model.config, question_encoder_tokenizer1, generator_tokenizer1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e07fb1-0cdd-4bb4-ad45-3d8001c5bcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-base\")\\nretriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-base\", index_name=\"exact\", use_dummy_dataset=True)\\nmodel = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-base\", retriever=retriever)\\nmodel = model.to(device)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-base\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-base\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-base\", retriever=retriever)\n",
    "model = model.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5d5ff-500b-4ce4-a500-6d0fc0dbce8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.conda-cont-base)",
   "language": "python",
   "name": "conda-env-.conda-cont-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
