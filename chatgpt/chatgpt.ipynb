{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6cc6f4-e947-4392-8090-e4788302347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b86fad-80b7-4cb1-bc7a-c2683762f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\" # Use your own API Key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4826edf1-00a1-4d5a-b85a-de8284a6dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_output_file = './chatgpt_responses/output.pickle'\n",
    "chat_output_bm_25_file = './chatgpt_responses/output_bm25.pickle'\n",
    "\n",
    "number_q_for_eval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd6c55c-8577-47ad-abfe-766099427af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(input_file):\n",
    "    with open(input_file, 'rb') as f:\n",
    "       out = pickle.load(f)\n",
    "    f.close()\n",
    "    return out\n",
    "\n",
    "def write_pickle(output, output_file):\n",
    "    with open(output_file, 'wb') as f:\n",
    "       pickle.dump(output, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8f0433-4037-45e7-a342-376ccd23f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bm25(): # these are actually the test files, before segregation into train val test\n",
    "    with open(\"dev_v2.1_query_details_passages\", \"rb\") as fp:   # Unpickling\n",
    "        query_details_passages = pickle.load(fp)\n",
    "\n",
    "    with open(\"passages.pickle\", \"rb\") as fp:   # Unpickling\n",
    "        passages = pickle.load(fp)\n",
    "\n",
    "    with open(\"dev_okapi_bm_25_top10\", \"rb\") as fp:   # Unpickling\n",
    "        dev_okapi_bm_25_top10 = pickle.load(fp)\n",
    "        \n",
    "    return (query_details_passages, passages, dev_okapi_bm_25_top10)\n",
    "\n",
    "def get_query_and_top_passages(queries, passages, bm25_top10, index):\n",
    "    query = queries[index]\n",
    "    passage_list = list(passages[i] for i in bm25_top10[index])\n",
    "    return (query, passage_list)\n",
    "    \n",
    "(queries, passages, bm25_top10) = read_bm25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ad26124-ee37-4fdc-aafd-978f56236e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor psg in queries[0]['passages']:\\n    print(psg)\\n    print()\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for psg in queries[0]['passages']:\n",
    "    print(psg)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fff1e15-efb9-4a99-b1fa-bfcb97ffeb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n(query, passage_list) = get_query_and_top_passages(queries, passages, bm25_top10, 2)\\nprint(query['query'])\\nprint()\\nprint(query['wellFormedAnswer'])\\nprint()\\nprint(query['selectedPassage'])\\nprint()\\nfor p in passage_list[0:10]:\\n    print(p)\\n    print()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "(query, passage_list) = get_query_and_top_passages(queries, passages, bm25_top10, 2)\n",
    "print(query['query'])\n",
    "print()\n",
    "print(query['wellFormedAnswer'])\n",
    "print()\n",
    "print(query['selectedPassage'])\n",
    "print()\n",
    "for p in passage_list[0:10]:\n",
    "    print(p)\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03f32355-8562-46c0-a403-10a8579d02af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you, as an AI language model, it is what I was designed to do and I strive to provide accurate and helpful responses.\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages = [\n",
    "     {\"role\": \"system\", \"content\" : \"Youâ€™re very good at comprehending reading comprehesions and finding answers to questions from them.\"}\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5648c03-bb8a-44ef-a1ac-670c60940c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_inference(chat_output_file, queries, passages, bm25_top10, number_queries=3, ground_truth=True):\n",
    "    chat_responses = []\n",
    "    for index in range(number_queries):\n",
    "        (query, ret_passages) = get_query_and_top_passages(queries, passages, bm25_top10, index) \n",
    "        passage_list = query['passages']\n",
    "        if (ground_truth == False):\n",
    "            passage_list = ret_passages\n",
    "        query_text = query['query']\n",
    "\n",
    "        if len(passage_list) == 10:\n",
    "            prompt = \"Given a question and 10 passages, find a well formed answer sentence. Do not mention the source in the 'answer'. \\n Q: {q} \\n P1: {p1} \\n P2: {p2} \\n P3: {p3} \\n P4: {p4} \\n P5: {p5} \\n P6: {p6} \\n P7: {p7} \\n P8: {p8} \\n P9: {p9} \\n P10: {p10} \\n\".format(q=query_text, p1=passage_list[0], p2=passage_list[1], p3=passage_list[2], p4=passage_list[3], p5=passage_list[4],\n",
    "            p6=passage_list[5], p7=passage_list[6], p8=passage_list[7], p9=passage_list[8], p10=passage_list[9])\n",
    "        elif len(passage_list) == 8:\n",
    "            # there are few outlier queries with just 8 grouth truth passages\n",
    "            prompt = \"Given a question and 8 passages, find a well formed answer sentence. Do not mention the source in the 'answer'. \\n Q: {q} \\n P1: {p1} \\n P2: {p2} \\n P3: {p3} \\n P4: {p4} \\n P5: {p5} \\n P6: {p6} \\n P7: {p7} \\n P8: {p8} \\n\".format(q=query_text, p1=passage_list[0], p2=passage_list[1], p3=passage_list[2], p4=passage_list[3], p5=passage_list[4],\n",
    "            p6=passage_list[5], p7=passage_list[6], p8=passage_list[7])\n",
    "        else:\n",
    "            raise Exception(\"Passage list len not compatible!!\")\n",
    "        #print(prompt)\n",
    "\n",
    "        input_dict = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        completion = openai.ChatCompletion.create(\n",
    "          model=\"gpt-3.5-turbo-0301\",\n",
    "          messages=input_dict\n",
    "        )\n",
    "\n",
    "        chat_response = completion.choices[0].message.content\n",
    "        #print(f'ChatGPT: {chat_response}')\n",
    "        #print()\n",
    "        chat_responses.append(chat_response)\n",
    "        time.sleep(21) # imp to add this as ChatGPT has a rate limit of 3 request/min, so in every 20s\n",
    "    print(len(chat_responses))\n",
    "    assert len(chat_responses) == number_queries\n",
    "    write_pickle(chat_responses, chat_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a02d437-5f09-4d38-90ed-690540ca8cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "chat_inference(chat_output_bm_25_file, queries, passages, bm25_top10, number_q_for_eval, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f2c5299-4a61-4044-9606-703c3a4666de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = read_pickle(chat_output_bm_25_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba4979d2-2aea-46d0-9d6c-101942a60ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04f9e012-3113-4f7b-9903-4112c882a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The population of Albany, MN according to the 2000 Census is not specified in any of the given passages.',\n",
       " 'There is no well formed answer sentence as none of the passages provide information about the current weather in Volcano, CA except for passage 7 which provides a detailed summary and forecast for the area.',\n",
       " 'Hippocrates is considered the father of modern medicine.',\n",
       " 'An appraisal for a Fannie loan is good for 120 days and must be current through the date of close, although there may be circumstances where the age of the appraisal may exceed this expiration under certain guidelines.',\n",
       " 'The average salary for pharmacy technicians can vary greatly depending on factors such as location, industry, experience, and benefits. According to various sources, the average salary ranges between $28,000 to $32,000 per year.',\n",
       " 'An average apple contains 58 calories.',\n",
       " 'On average, a central air conditioning system can last for about 15 years, but the actual lifespan can vary depending on factors such as maintenance, usage, and the specific make and model of the system.',\n",
       " 'The summer hours of operation for the Animal Rescue League are Monday 11am-5pm, Tuesday 11am-8pm, Wednesday closed, Thursday 11am-8pm, Friday 11am-5pm, and Saturday 11am-4pm.',\n",
       " 'The name Ellianna means \"My lord has answered\" and is a Modern English variant of the Hebrew name Eliana. It is primarily used as a girls name and is not very popular, although it ranked 1,043rd in the list of most popular baby girl names in the US in 2019.',\n",
       " 'A: The flight duration from DFW to Punta Cana is not provided in the given passages.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0766ac-1212-4755-a83e-08f115e1e96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.conda-cont-base)",
   "language": "python",
   "name": "conda-env-.conda-cont-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
